{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29938804/generate-random-sentences-in-python\n",
    "# Use variable like subjct = \"john\",verb = \"ate\",object = \"an apple\"\n",
    "#Option1\n",
    "import random\n",
    "\n",
    "nouns = (\"puppy\", \"car\", \"rabbit\", \"girl\", \"monkey\")\n",
    "verbs = (\"runs\", \"hits\", \"jumps\", \"drives\", \"barfs\") \n",
    "adv = (\"crazily.\", \"dutifully.\", \"foolishly.\", \"merrily.\", \"occasionally.\")\n",
    "adj = (\"adorable\", \"clueless\", \"dirty\", \"odd\", \"stupid\")\n",
    "num = random.randrange(0,5)\n",
    "print (nouns[num] + ' ' + verbs[num] + ' ' + adv[num] + ' ' + adj[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29938804/generate-random-sentences-in-python\n",
    "#Option2\n",
    "import random\n",
    "\n",
    "nouns = (\"puppy\", \"car\", \"rabbit\", \"girl\", \"monkey\")\n",
    "verbs = (\"runs\", \"hits\", \"jumps\", \"drives\", \"barfs\") \n",
    "adv = (\"crazily.\", \"dutifully.\", \"foolishly.\", \"merrily.\", \"occasionally.\")\n",
    "adj = (\"adorable\", \"clueless\", \"dirty\", \"odd\", \"stupid\")\n",
    "num = random.randrange(0,5)\n",
    "l=[nouns,verbs,adj,adv]\n",
    "' '.join([random.choice(i) for i in l])\n",
    "print (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option3\n",
    "#http://pythonfiddle.com/random-sentence-generator/\n",
    "import random\n",
    "\n",
    "s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"retards\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"retard\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "\n",
    "def sing_sen_maker():\n",
    "    '''Makes a random senctence from the different parts of speech. Uses a SINGULAR subject'''\n",
    "    if input(\"Would you like to add a new word?\").lower() == \"yes\":\n",
    "        new_word = input(\"Please enter a singular noun.\")\n",
    "        s_nouns.append(new_word)\n",
    "    else:\n",
    "        print (random.choice(s_nouns), random.choice(s_verbs), random.choice(s_nouns).lower() or random.choice(p_nouns).lower(), random.choice(infinitives))\n",
    "\n",
    "        \n",
    "        \n",
    "print (sing_sen_maker())        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabeth sings\n",
      "dog barks\n",
      "Eiffel Tower shines\n",
      "Bike shines\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/36912106/meaningful-sentence-generation-from-words-which-are-classified-as-per-parts-of-s\n",
    "# #Option3\n",
    "eli = {'CAT': 'N', 'ORTH': 'Elizabeth', 'FEAT':'human'}\n",
    "dog = {'CAT': 'N', 'ORTH': 'dog', 'FEAT':'animal'}\n",
    "eiffel = {'CAT': 'N', 'ORTH': 'Eiffel Tower', 'FEAT':'inanimate'}\n",
    "bike = {'CAT': 'N', 'ORTH': 'Bike', 'FEAT':'inanimate'}\n",
    "\n",
    "nouns = [eli, dog, eiffel, bike]\n",
    "\n",
    "sings = {'CAT': 'V', 'ORTH': 'sings', 'FEAT':'human'}\n",
    "barks = {'CAT': 'V', 'ORTH': 'barks', 'FEAT':'animal'}\n",
    "shines = {'CAT': 'V', 'ORTH': 'shines', 'FEAT':'inanimate'}\n",
    "\n",
    "verbs = [sings, barks, shines]\n",
    "\n",
    "# Our sentence pattern is: noun + verb + noun\n",
    "\n",
    "for n in nouns:\n",
    "    for v in verbs:\n",
    "        if n['FEAT'] == v['FEAT']:\n",
    "            print('{} {}'.format(n['ORTH'], v['ORTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of']\n"
     ]
    }
   ],
   "source": [
    "#http://www.ramtin.xyz/post/4\n",
    "# #Option4\n",
    "import random \n",
    "model = {'START': ['i', 'we', 'you'], 'i': ['like'], 'like': ['to'], 'to': ['eat'], 'you': ['eat'], 'we': ['eat'], 'eat': ['apples', 'oranges', 'apples'], 'END': ['apples', 'oranges', 'apples']}\n",
    "dataset_file = open(\"/Users/kumarsanjeev/Desktop/Test2.txt\")\n",
    "for line in dataset_file:    #dataset_file is a txt file with training quotes \n",
    "    line = line.lower().split()\n",
    "    for i, word in enumerate(line):\n",
    "        if i == len(line)-1:   \n",
    "            model['END'] = model.get('END', []) + [word]\n",
    "           \n",
    "        else:    \n",
    "            if i == 0:\n",
    "                model['START'] = model.get('START', []) + [word]\n",
    "            model[word] = model.get(word, []) + [line[i+1]]\n",
    "           \n",
    "            \n",
    "            \n",
    "\n",
    "generated = []\n",
    "while True:\n",
    "    if not generated:\n",
    "        words = model['START']\n",
    "    elif generated[-1] in model['END']:\n",
    "        break\n",
    "    else:\n",
    "        words = model[generated[-1]]\n",
    "    generated.append(random.choice(words))   \n",
    "    \n",
    "print (generated)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-245b97c78ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mverbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnoun1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FEAT'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FEAT'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FEAT'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FEAT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} {} {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "eli = {'CAT': 'N', 'ORTH': 'Elizabeth', 'FEAT':'human'}\n",
    "\n",
    "\n",
    "nouns = [eli]\n",
    "\n",
    "sings = {'CAT': 'V', 'ORTH': 'sings', 'FEAT':'human'}\n",
    "\n",
    "\n",
    "verbs = [sings]\n",
    "\n",
    "jon = {'CAT': 'M', 'ORTH': 'John', 'FEAT':'human'}\n",
    "\n",
    "noun1 = ['John']\n",
    "# Our sentence pattern is: noun + verb + noun\n",
    "\n",
    "for n in nouns:\n",
    "    for v in verbs:\n",
    "        for k in noun1:\n",
    "            if n['FEAT'] == v['FEAT'] and n['FEAT'] == k['FEAT']:\n",
    "                print('{} {} {}'.format(n['ORTH'], v['ORTH'],k['ORTH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to add a new word?No\n",
      "Superman retards my mom because the sky is green.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"retards\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"retard\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "\n",
    "def sing_sen_maker():\n",
    "    '''Makes a random senctence from the different parts of speech. Uses a SINGULAR subject'''\n",
    "    if input(\"Would you like to add a new word?\").lower() == \"yes\":\n",
    "        new_word = input(\"Please enter a singular noun.\")\n",
    "        s_nouns.append(new_word)\n",
    "    else:\n",
    "    \tprint (random.choice(s_nouns), random.choice(s_verbs), random.choice(s_nouns).lower() or random.choice(p_nouns).lower(), random.choice(infinitives))\n",
    "\n",
    "        \n",
    "        \n",
    "print (sing_sen_maker())        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/hrs/markov-sentence-generator\n",
    "#!/usr/bin/python\n",
    "\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# These mappings can get fairly large -- they're stored globally to\n",
    "# save copying time.\n",
    "\n",
    "# (tuple of words) -> {dict: word -> number of times the word appears following the tuple}\n",
    "# Example entry:\n",
    "#    ('eyes', 'turned') => {'to': 2.0, 'from': 1.0}\n",
    "# Used briefly while first constructing the normalized mapping\n",
    "tempMapping = {}\n",
    "\n",
    "# (tuple of words) -> {dict: word -> *normalized* number of times the word appears following the tuple}\n",
    "# Example entry:\n",
    "#    ('eyes', 'turned') => {'to': 0.66666666, 'from': 0.33333333}\n",
    "mapping = {}\n",
    "\n",
    "# Contains the set of words that can start sentences\n",
    "starts = []\n",
    "\n",
    "# We want to be able to compare words independent of their capitalization.\n",
    "def fixCaps(word):\n",
    "    # Ex: \"FOO\" -> \"foo\"\n",
    "    if word.isupper() and word != \"I\":\n",
    "        word = word.lower()\n",
    "        # Ex: \"LaTeX\" => \"Latex\"\n",
    "    elif word [0].isupper():\n",
    "        word = word.lower().capitalize()\n",
    "        # Ex: \"wOOt\" -> \"woot\"\n",
    "    else:\n",
    "        word = word.lower()\n",
    "    return word\n",
    "\n",
    "# Tuples can be hashed; lists can't.  We need hashable values for dict keys.\n",
    "# This looks like a hack (and it is, a little) but in practice it doesn't\n",
    "# affect processing time too negatively.\n",
    "def toHashKey(lst):\n",
    "    return tuple(lst)\n",
    "\n",
    "# Returns the contents of the file, split into a list of words and\n",
    "# (some) punctuation.\n",
    "def wordlist(filename):\n",
    "    f = open(\"/Users/kumarsanjeev/Desktop/Test1.txt\", 'r')\n",
    "    wordlist = [fixCaps(w) for w in re.findall(r\"[\\w']+|[.,!?;]\", f.read())]\n",
    "    f.close()\n",
    "    return wordlist\n",
    "\n",
    "\n",
    "\n",
    "# Self-explanatory -- adds \"word\" to the \"tempMapping\" dict under \"history\".\n",
    "# tempMapping (and mapping) both match each word to a list of possible next\n",
    "# words.\n",
    "# Given history = [\"the\", \"rain\", \"in\"] and word = \"Spain\", we add \"Spain\" to\n",
    "# the entries for [\"the\", \"rain\", \"in\"], [\"rain\", \"in\"], and [\"in\"].\n",
    "def addItemToTempMapping(history, word):\n",
    "    global tempMapping\n",
    "    while len(history) > 0:\n",
    "        first = toHashKey(history)\n",
    "        if first in tempMapping:\n",
    "            if word in tempMapping[first]:\n",
    "                tempMapping[first][word] += 1.0\n",
    "            else:\n",
    "                tempMapping[first][word] = 1.0\n",
    "        else:\n",
    "            tempMapping[first] = {}\n",
    "            tempMapping[first][word] = 1.0\n",
    "        history = history[1:]\n",
    "\n",
    "# Building and normalizing the mapping.\n",
    "def buildMapping(wordlist, markovLength):\n",
    "    global tempMapping\n",
    "    starts.append(wordlist [0])\n",
    "    for i in range(1, len(wordlist) - 1):\n",
    "        if i <= markovLength:\n",
    "            history = wordlist[: i + 1]\n",
    "        else:\n",
    "            history = wordlist[i - markovLength + 1 : i + 1]\n",
    "        follow = wordlist[i + 1]\n",
    "        # if the last elt was a period, add the next word to the start list\n",
    "        if history[-1] == \".\" and follow not in \".,!?;\":\n",
    "            starts.append(follow)\n",
    "        addItemToTempMapping(history, follow)\n",
    "    # Normalize the values in tempMapping, put them into mapping\n",
    "    for first, followset in tempMapping.iteritems():\n",
    "        total = sum(followset.values())\n",
    "        # Normalizing here:\n",
    "        mapping[first] = dict([(k, v / total) for k, v in followset.iteritems()])\n",
    "\n",
    "# Returns the next word in the sentence (chosen randomly),\n",
    "# given the previous ones.\n",
    "def next(prevList):\n",
    "    sum = 0.0\n",
    "    retval = \"\"\n",
    "    index = random.random()\n",
    "    # Shorten prevList until it's in mapping\n",
    "    while toHashKey(prevList) not in mapping:\n",
    "        prevList.pop(0)\n",
    "    # Get a random word from the mapping, given prevList\n",
    "    for k, v in mapping[toHashKey(prevList)].iteritems():\n",
    "        sum += v\n",
    "        if sum >= index and retval == \"\":\n",
    "            retval = k\n",
    "    return retval\n",
    "\n",
    "def genSentence(markovLength):\n",
    "    # Start with a random \"starting word\"\n",
    "    curr = random.choice(starts)\n",
    "    sent = curr.capitalize()\n",
    "    prevList = [curr]\n",
    "    # Keep adding words until we hit a period\n",
    "    while (curr not in \".\"):\n",
    "        curr = next(prevList)\n",
    "        prevList.append(curr)\n",
    "        # if the prevList has gotten too long, trim it\n",
    "        if len(prevList) > markovLength:\n",
    "            prevList.pop(0)\n",
    "        if (curr not in \".,!?;\"):\n",
    "            sent += \" \" # Add spaces between words (but not punctuation)\n",
    "        sent += curr\n",
    "    return sent\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        sys.stderr.write('Usage: ' + sys.argv [0] + ' text_source [chain_length=1]\\n')\n",
    "        sys.exit(1)\n",
    "\n",
    "    filename = sys.argv[1]\n",
    "    markovLength = 1\n",
    "    if len (sys.argv) == 3:\n",
    "        markovLength = int(sys.argv [2])\n",
    "\n",
    "    buildMapping(wordlist(filename), markovLength)\n",
    "    print (genSentence(markovLength))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'spell', 'of', 'arms', 'and', 'voices', 'the', 'white', 'arms', 'of', 'roads', ',', 'their', 'promise', 'of', 'close', 'embraces', 'and', 'the', 'black', 'arms', 'of', 'tall', 'ships', 'that', 'stand', 'against', 'the', 'moon', ',', 'their', 'tale', 'of', 'distant', 'nations', '.', 'They', 'are', 'held', 'out', 'to', 'say', 'We', 'are', 'alone', 'come', '.', 'And', 'the', 'voices', 'say', 'with', 'them', 'We', 'are', 'your', 'kinsmen', '.', 'And', 'the', 'air', 'is', 'thick', 'with', 'their', 'company', 'as', 'they', 'call', 'to', 'me', ',', 'their', 'kinsman', ',', 'making', 'ready', 'to', 'go', ',', 'shaking', 'the', 'wings', 'of', 'their', 'exultant', 'and', 'terrible', 'youth', '.', 'Mother', 'is', 'putting', 'my', 'new', 'secondhand', 'clothes', 'in', 'order', '.', 'She', 'prays', 'now', ',', 'she', 'says', ',', 'that', 'I', 'may', 'learn', 'in', 'my', 'own', 'life', 'and', 'away', 'from', 'home', 'and', 'friends', 'what', 'the', 'heart', 'is', 'and', 'what', 'it', 'feels', '.', 'Amen', '.', 'So', 'be', 'it', '.', 'Welcome', ',', 'o', 'life', ',', 'I', 'go', 'to', 'encounter', 'for', 'the', 'millionth', 'time', 'the', 'reality', 'of', 'experience', 'and', 'to', 'forge', 'in', 'the', 'smithy', 'of', 'my', 'soul', 'the', 'uncreated', 'conscience', 'of', 'my', 'race', '.']\n"
     ]
    }
   ],
   "source": [
    "def wordlist(filename):\n",
    "    f = open(filename, 'r')\n",
    "    wordlist = [fixCaps(w) for w in re.findall(r\"[\\w']+|[.,!?;]\", f.read())]\n",
    "    f.close()\n",
    "    return wordlist\n",
    "\n",
    "\n",
    "test = \"/Users/kumarsanjeev/Desktop/Test1.txt\"\n",
    "test1 = \"/Users/kumarsanjeev/Desktop/Test1.txt\"\n",
    "print(wordlist(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '/Users/kumarsanjeev/Library/Jupyter/runtime/kernel-ecd89723-815b-4845-a082-8e4b6a199c0a.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e5d42e53c908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-e5d42e53c908>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mmarkovLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mmarkovLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mbuildMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkovLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/Users/kumarsanjeev/Library/Jupyter/runtime/kernel-ecd89723-815b-4845-a082-8e4b6a199c0a.json'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-53894bc9146a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-53894bc9146a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mmarkovLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mmarkovLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mbuildMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkovLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Nov  9 20:33:01 2018\n",
    "\n",
    "@author: kumarsanjeev\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# These mappings can get fairly large -- they're stored globally to\n",
    "# save copying time.\n",
    "\n",
    "# (tuple of words) -> {dict: word -> number of times the word appears following the tuple}\n",
    "# Example entry:\n",
    "#    ('eyes', 'turned') => {'to': 2.0, 'from': 1.0}\n",
    "# Used briefly while first constructing the normalized mapping\n",
    "tempMapping = {}\n",
    "\n",
    "# (tuple of words) -> {dict: word -> *normalized* number of times the word appears following the tuple}\n",
    "# Example entry:\n",
    "#    ('eyes', 'turned') => {'to': 0.66666666, 'from': 0.33333333}\n",
    "mapping = {}\n",
    "\n",
    "# Contains the set of words that can start sentences\n",
    "starts = []\n",
    "\n",
    "# We want to be able to compare words independent of their capitalization.\n",
    "\n",
    "\n",
    "def fixCaps(word):\n",
    "    # Ex: \"FOO\" -> \"foo\"\n",
    "    if word.isupper() and word != \"I\":\n",
    "        word = word.lower()\n",
    "        # Ex: \"LaTeX\" => \"Latex\"\n",
    "    elif word [0].isupper():\n",
    "        word = word.lower().capitalize()\n",
    "        # Ex: \"wOOt\" -> \"woot\"\n",
    "    else:\n",
    "        word = word.lower()\n",
    "    return word\n",
    "\n",
    "# Tuples can be hashed; lists can't.  We need hashable values for dict keys.\n",
    "# This looks like a hack (and it is, a little) but in practice it doesn't\n",
    "# affect processing time too negatively.\n",
    "\n",
    "\n",
    "def toHashKey(lst):\n",
    "    return tuple(lst)\n",
    "\n",
    "# Returns the contents of the file, split into a list of words and\n",
    "# (some) punctuation.\n",
    "\n",
    "\n",
    "def wordlist(filename):\n",
    "    f = open(filename, 'r')\n",
    "    wordlist = [fixCaps(w) for w in re.findall(r\"[\\w']+|[.,!?;]\", f.read())]\n",
    "    f.close()\n",
    "    return wordlist\n",
    "\n",
    "\n",
    "\n",
    "# Self-explanatory -- adds \"word\" to the \"tempMapping\" dict under \"history\".\n",
    "# tempMapping (and mapping) both match each word to a list of possible next\n",
    "# words.\n",
    "# Given history = [\"the\", \"rain\", \"in\"] and word = \"Spain\", we add \"Spain\" to\n",
    "# the entries for [\"the\", \"rain\", \"in\"], [\"rain\", \"in\"], and [\"in\"].\n",
    "\n",
    "\n",
    "def addItemToTempMapping(history, word):\n",
    "    global tempMapping\n",
    "    while len(history) > 0:\n",
    "        first = toHashKey(history)\n",
    "        if first in tempMapping:\n",
    "            if word in tempMapping[first]:\n",
    "                tempMapping[first][word] += 1.0\n",
    "            else:\n",
    "                tempMapping[first][word] = 1.0\n",
    "        else:\n",
    "            tempMapping[first] = {}\n",
    "            tempMapping[first][word] = 1.0\n",
    "        history = history[1:]\n",
    "\n",
    "# Building and normalizing the mapping.\n",
    "\n",
    "\n",
    "def buildMapping(wordlist, markovLength):\n",
    "    global tempMapping\n",
    "    starts.append(wordlist [0])\n",
    "    for i in range(1, len(wordlist) - 1):\n",
    "        if i <= markovLength:\n",
    "            history = wordlist[: i + 1]\n",
    "        else:\n",
    "            history = wordlist[i - markovLength + 1 : i + 1]\n",
    "        follow = wordlist[i + 1]\n",
    "        # if the last elt was a period, add the next word to the start list\n",
    "        if history[-1] == \".\" and follow not in \".,!?;\":\n",
    "            starts.append(follow)\n",
    "        addItemToTempMapping(history, follow)\n",
    "    # Normalize the values in tempMapping, put them into mapping\n",
    "    for first, followset in tempMapping.items():\n",
    "        total = sum(followset.values())\n",
    "        # Normalizing here:\n",
    "        mapping[first] = dict([(k, v / total) for k, v in followset.items()])\n",
    "\n",
    "# Returns the next word in the sentence (chosen randomly),\n",
    "# given the previous ones.\n",
    "\n",
    "\n",
    "def next(prevList):\n",
    "    sum = 0.0\n",
    "    retval = \"\"\n",
    "    index = random.random()\n",
    "    # Shorten prevList until it's in mapping\n",
    "    while toHashKey(prevList) not in mapping:\n",
    "        prevList.pop(0)\n",
    "    # Get a random word from the mapping, given prevList\n",
    "    for k, v in mapping[toHashKey(prevList)].items():\n",
    "        sum += v\n",
    "        if sum >= index and retval == \"\":\n",
    "            retval = k\n",
    "    return retval\n",
    "\n",
    "\n",
    "def genSentence(markovLength):\n",
    "    # Start with a random \"starting word\"\n",
    "    curr = random.choice(starts)\n",
    "    sent = curr.capitalize()\n",
    "    prevList = [curr]\n",
    "    # Keep adding words until we hit a period\n",
    "    while (curr not in \".\"):\n",
    "        curr = next(prevList)\n",
    "        prevList.append(curr)\n",
    "        # if the prevList has gotten too long, trim it\n",
    "        if len(prevList) > markovLength:\n",
    "            prevList.pop(0)\n",
    "        if (curr not in \".,!?;\"):\n",
    "            sent += \" \" # Add spaces between words (but not punctuation)\n",
    "        sent += curr\n",
    "    return sent\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        sys.stderr.write('Usage: ' + sys.argv [0] + ' text_source [chain_length=1]\\n')\n",
    "        sys.exit(1)\n",
    "\n",
    "    filename = sys.argv[1]\n",
    "    markovLength = 1\n",
    "    if len (sys.argv) == 3:\n",
    "        markovLength = int(sys.argv [3])\n",
    "\n",
    "    buildMapping(wordlist(filename), markovLength)\n",
    "    print (genSentence(markovLength))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "nouns = (\"puppy\", \"car\", \"rabbit\", \"girl\", \"monkey\")\n",
    "verbs = (\"runs\", \"hits\", \"jumps\", \"drives\", \"barfs\") \n",
    "adv = (\"crazily.\", \"dutifully.\", \"foolishly.\", \"merrily.\", \"occasionally.\")\n",
    "adj = (\"adorable\", \"clueless\", \"dirty\", \"odd\", \"stupid\")\n",
    "num = random.randrange(0,5)\n",
    "print (num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'eat', 'apples']\n"
     ]
    }
   ],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning model...\n",
      "Sampling...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-175632c3a773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of weights does not match the population'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mbisect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bisect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbisect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcum_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbisect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcum_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# This is the length of the \"state\" the current character is predicted from.\n",
    "# For Markov chains with memory, this is the \"order\" of the chain. For n-grams,\n",
    "# n is STATE_LEN+1 since it includes the predicted character as well.\n",
    "STATE_LEN = 4\n",
    "\n",
    "data = \"Hi how are you? I stay in Delhi.you are good. Hello world.hi.\"\n",
    "model = defaultdict(Counter)\n",
    "\n",
    "print('Learning model...')\n",
    "for i in range(len(data) - STATE_LEN):\n",
    "    state = data[i:i + STATE_LEN]\n",
    "    next = data[i + STATE_LEN]\n",
    "    model[state][next] += 1\n",
    "\n",
    "print('Sampling...')\n",
    "state = random.choice(list(model))\n",
    "out = list(state)\n",
    "for i in range(400):\n",
    "    out.extend(random.choices(list(model[state]), model[state].values()))\n",
    "    state = state[1:] + out[-1]\n",
    "print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-53894bc9146a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-53894bc9146a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mmarkovLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mmarkovLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mbuildMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkovLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Nov  9 20:33:01 2018\n",
    "\n",
    "@author: kumarsanjeev\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# These mappings can get fairly large -- they're stored globally to\n",
    "# save copying time.\n",
    "\n",
    "# (tuple of words) -> {dict: word -> number of times the word appears following the tuple}\n",
    "# Example entry:\n",
    "#    ('eyes', 'turned') => {'to': 2.0, 'from': 1.0}\n",
    "# Used briefly while first constructing the normalized mapping\n",
    "tempMapping = {}\n",
    "\n",
    "# (tuple of words) -> {dict: word -> *normalized* number of times the word appears following the tuple}\n",
    "# Example entry:\n",
    "#    ('eyes', 'turned') => {'to': 0.66666666, 'from': 0.33333333}\n",
    "mapping = {}\n",
    "\n",
    "# Contains the set of words that can start sentences\n",
    "starts = []\n",
    "\n",
    "# We want to be able to compare words independent of their capitalization.\n",
    "\n",
    "\n",
    "def fixCaps(word):\n",
    "    # Ex: \"FOO\" -> \"foo\"\n",
    "    if word.isupper() and word != \"I\":\n",
    "        word = word.lower()\n",
    "        # Ex: \"LaTeX\" => \"Latex\"\n",
    "    elif word [0].isupper():\n",
    "        word = word.lower().capitalize()\n",
    "        # Ex: \"wOOt\" -> \"woot\"\n",
    "    else:\n",
    "        word = word.lower()\n",
    "    return word\n",
    "\n",
    "# Tuples can be hashed; lists can't.  We need hashable values for dict keys.\n",
    "# This looks like a hack (and it is, a little) but in practice it doesn't\n",
    "# affect processing time too negatively.\n",
    "\n",
    "\n",
    "def toHashKey(lst):\n",
    "    return tuple(lst)\n",
    "\n",
    "# Returns the contents of the file, split into a list of words and\n",
    "# (some) punctuation.\n",
    "\n",
    "\n",
    "def wordlist(filename):\n",
    "    f = open(filename, 'r')\n",
    "    wordlist = [fixCaps(w) for w in re.findall(r\"[\\w']+|[.,!?;]\", f.read())]\n",
    "    f.close()\n",
    "    return wordlist\n",
    "\n",
    "\n",
    "\n",
    "# Self-explanatory -- adds \"word\" to the \"tempMapping\" dict under \"history\".\n",
    "# tempMapping (and mapping) both match each word to a list of possible next\n",
    "# words.\n",
    "# Given history = [\"the\", \"rain\", \"in\"] and word = \"Spain\", we add \"Spain\" to\n",
    "# the entries for [\"the\", \"rain\", \"in\"], [\"rain\", \"in\"], and [\"in\"].\n",
    "\n",
    "\n",
    "def addItemToTempMapping(history, word):\n",
    "    global tempMapping\n",
    "    while len(history) > 0:\n",
    "        first = toHashKey(history)\n",
    "        if first in tempMapping:\n",
    "            if word in tempMapping[first]:\n",
    "                tempMapping[first][word] += 1.0\n",
    "            else:\n",
    "                tempMapping[first][word] = 1.0\n",
    "        else:\n",
    "            tempMapping[first] = {}\n",
    "            tempMapping[first][word] = 1.0\n",
    "        history = history[1:]\n",
    "\n",
    "# Building and normalizing the mapping.\n",
    "\n",
    "\n",
    "def buildMapping(wordlist, markovLength):\n",
    "    global tempMapping\n",
    "    starts.append(wordlist [0])\n",
    "    for i in range(1, len(wordlist) - 1):\n",
    "        if i <= markovLength:\n",
    "            history = wordlist[: i + 1]\n",
    "        else:\n",
    "            history = wordlist[i - markovLength + 1 : i + 1]\n",
    "        follow = wordlist[i + 1]\n",
    "        # if the last elt was a period, add the next word to the start list\n",
    "        if history[-1] == \".\" and follow not in \".,!?;\":\n",
    "            starts.append(follow)\n",
    "        addItemToTempMapping(history, follow)\n",
    "    # Normalize the values in tempMapping, put them into mapping\n",
    "    for first, followset in tempMapping.items():\n",
    "        total = sum(followset.values())\n",
    "        # Normalizing here:\n",
    "        mapping[first] = dict([(k, v / total) for k, v in followset.items()])\n",
    "\n",
    "# Returns the next word in the sentence (chosen randomly),\n",
    "# given the previous ones.\n",
    "\n",
    "\n",
    "def next(prevList):\n",
    "    sum = 0.0\n",
    "    retval = \"\"\n",
    "    index = random.random()\n",
    "    # Shorten prevList until it's in mapping\n",
    "    while toHashKey(prevList) not in mapping:\n",
    "        prevList.pop(0)\n",
    "    # Get a random word from the mapping, given prevList\n",
    "    for k, v in mapping[toHashKey(prevList)].items():\n",
    "        sum += v\n",
    "        if sum >= index and retval == \"\":\n",
    "            retval = k\n",
    "    return retval\n",
    "\n",
    "\n",
    "def genSentence(markovLength):\n",
    "    # Start with a random \"starting word\"\n",
    "    curr = random.choice(starts)\n",
    "    sent = curr.capitalize()\n",
    "    prevList = [curr]\n",
    "    # Keep adding words until we hit a period\n",
    "    while (curr not in \".\"):\n",
    "        curr = next(prevList)\n",
    "        prevList.append(curr)\n",
    "        # if the prevList has gotten too long, trim it\n",
    "        if len(prevList) > markovLength:\n",
    "            prevList.pop(0)\n",
    "        if (curr not in \".,!?;\"):\n",
    "            sent += \" \" # Add spaces between words (but not punctuation)\n",
    "        sent += curr\n",
    "    return sent\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        sys.stderr.write('Usage: ' + sys.argv [0] + ' text_source [chain_length=1]\\n')\n",
    "        sys.exit(1)\n",
    "\n",
    "    filename = sys.argv[1]\n",
    "    markovLength = 1\n",
    "    if len (sys.argv) == 3:\n",
    "        markovLength = int(sys.argv [3])\n",
    "\n",
    "    buildMapping(wordlist(filename), markovLength)\n",
    "    print (genSentence(markovLength))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not _io.TextIOWrapper",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2b3302905b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/kumarsanjeev/Desktop/Test1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreaddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-2b3302905b93>\u001b[0m in \u001b[0;36mreaddata\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreaddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'''Read file and return contents.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not _io.TextIOWrapper"
     ]
    }
   ],
   "source": [
    "def readdata(file):\n",
    "    '''Read file and return contents.'''\n",
    "    with open(file) as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "\n",
    "f = open(\"/Users/kumarsanjeev/Desktop/Test1.txt\", 'r')\n",
    "print (readdata(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burnt, and balanced an army or a deep, hollow tone: 'it means-to-make-anything-prettier.' 'Well, it made it by talking to the next day, but when I shall never happened, she get back to wonder if I come on each a safe to Alice. 'Now at processions; 'and the strangest of the Palace and other side until their never-ending meal, and as he made him enter the suppressed guinea-pigs, filled the raft after glaring at all else to do not easy to find her gently on her. Who will give me back to carry your slaves? asked the Scarecrow. The Dormouse began smoking a minute, 'and she was the Scarecrow sat down on mourning; and terrible; yet you that.' 'Well, I never been jumping up again! \n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    " \n",
    "\n",
    "document= \"/Users/kumarsanjeev/desktop/Test1 _SentGeneration.txt\"\n",
    "def readdata(file):\n",
    "    '''Read file and return contents.'''\n",
    "    with open(file) as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    " \n",
    " \n",
    "def makerule(data, context):\n",
    "    '''Make a rule dict for given data.'''\n",
    "    rule = {}\n",
    "    words = data.split(' ')\n",
    "    index = context\n",
    " \n",
    "    for word in words[index:]:\n",
    "        key = ' '.join(words[index-context:index])\n",
    "        if key in rule:\n",
    "            rule[key].append(word)\n",
    "        else:\n",
    "            rule[key] = [word]\n",
    "        index += 1\n",
    " \n",
    "    return rule\n",
    " \n",
    " \n",
    "def makestring(rule, length):    \n",
    "    '''Use a given rule to make a string.'''\n",
    "    oldwords = random.choice(list(rule.keys())).split(' ') #random starting words\n",
    "    string = ' '.join(oldwords) + ' '\n",
    " \n",
    "    for i in range(length):\n",
    "        try:\n",
    "            key = ' '.join(oldwords)\n",
    "            newword = random.choice(rule[key])\n",
    "            string += newword + ' '\n",
    " \n",
    "            for word in range(len(oldwords)):\n",
    "                oldwords[word] = oldwords[(word + 1) % len(oldwords)]\n",
    "            oldwords[-1] = newword\n",
    " \n",
    "        except KeyError:\n",
    "            return string\n",
    "    return string\n",
    " \n",
    " \n",
    "p = readdata(document)\n",
    "#print(p)\n",
    "rule = makerule(p , 1)\n",
    "#print(rule) \n",
    "print(makestring(rule , len(rule)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
