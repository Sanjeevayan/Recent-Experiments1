{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer [Synset('computer.n.01'), Synset('calculator.n.01')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "a = \"I want to have  a computer \"\n",
    "b = word_tokenize(a)\n",
    "\n",
    "for item in b:\n",
    "\n",
    "    if item == \"computer\":\n",
    "        c = wordnet.synsets(item)\n",
    "        print (item,c,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('privation.n.01'), Synset('lack.n.01'), Synset('need.n.02'), Synset('wish.n.01'), Synset('desire.v.01'), Synset('want.v.02'), Synset('want.v.03'), Synset('want.v.04'), Synset('want.v.05')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "fruit = 'want'\n",
    "wn.synsets(fruit)\n",
    "a = wn.synsets(fruit)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('analog_computer.n.01'),\n",
       " Synset('digital_computer.n.01'),\n",
       " Synset('home_computer.n.01'),\n",
       " Synset('node.n.08'),\n",
       " Synset('number_cruncher.n.02'),\n",
       " Synset('pari-mutuel_machine.n.01'),\n",
       " Synset('predictor.n.03'),\n",
       " Synset('server.n.03'),\n",
       " Synset('turing_machine.n.01'),\n",
       " Synset('web_site.n.01')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.hypernyms()\n",
    "a.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London --> GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I went to London.')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"-->\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It --> It\n",
      "originated --> origin\n",
      "from --> from\n",
      "the --> the\n",
      "idea --> idea\n",
      "that --> that\n",
      "there --> there\n",
      "are --> are\n",
      "readers --> reader\n",
      "who --> who\n",
      "prefer --> prefer\n",
      "learning --> learn\n",
      "new --> new\n",
      "skills --> skill\n",
      "from --> from\n",
      "the --> the\n",
      "comforts --> comfort\n",
      "of --> of\n",
      "their --> their\n",
      "drawing --> draw\n",
      "rooms --> room\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "text = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "tokens = word_tokenize(text)\n",
    "for w in tokens:\n",
    "    #print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))\n",
    "    print (w,\"-->\",porter_stemmer.stem(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It -- It\n",
      "originated -- originated\n",
      "from -- from\n",
      "the -- the\n",
      "idea -- idea\n",
      "that -- that\n",
      "there -- there\n",
      "are -- are\n",
      "readers -- reader\n",
      "who -- who\n",
      "prefer -- prefer\n",
      "learning -- learning\n",
      "new -- new\n",
      "skills -- skill\n",
      "from -- from\n",
      "the -- the\n",
      "comforts -- comfort\n",
      "of -- of\n",
      "their -- their\n",
      "drawing -- drawing\n",
      "rooms -- room\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "tokens = word_tokenize(text)\n",
    "for token in tokens:\n",
    "    print (token,\"--\",lemmatizer.lemmatize(token))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
