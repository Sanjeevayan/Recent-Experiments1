{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLU:Entity Extraction & Subject-Predicate-Object Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want a new Laptop.\n",
      "Subject,Predicate, Object are: [' I', 'want', 'Laptop']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#doc = nlp(u\"The Boys want to reset domain password.\")\n",
    "stop_word = [\"a\",\"an\",\"the\",\"my\",\"new\"]\n",
    "string = input()\n",
    "new_doc = ''\n",
    "for i in string.split():\n",
    "    if i in stop_word:\n",
    "        continue\n",
    "    else :\n",
    "        new_doc = new_doc+' '+i\n",
    "doc1 = nlp(new_doc)\n",
    "\n",
    "list1 = []      \n",
    "for chunk in doc1.noun_chunks:\n",
    "    list1.append(chunk.text)\n",
    "list1.append(chunk.root.head.text)\n",
    "list1[-1] ,list1[-2] = list1[-2] ,list1[-1]\n",
    "\n",
    "print(\"Subject,Predicate, Object are:\",list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology Parsing and Entity Lookup in Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"dell\": {\n",
      "        \"desc\": \"\",\n",
      "        \"id\": \"dell\",\n",
      "        \"name\": \"\",\n",
      "        \"other\": {},\n",
      "        \"relations\": {\n",
      "            \"is_a\": [\n",
      "                \"laptop\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"hp\": {\n",
      "        \"desc\": \"\",\n",
      "        \"id\": \"hp\",\n",
      "        \"name\": \"\",\n",
      "        \"other\": {},\n",
      "        \"relations\": {\n",
      "            \"is_a\": [\n",
      "                \"laptop\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"laptop\": {\n",
      "        \"desc\": \"\",\n",
      "        \"id\": \"laptop\",\n",
      "        \"name\": \"\",\n",
      "        \"other\": {},\n",
      "        \"relations\": {\n",
      "            \"can_be\": [\n",
      "                \"dell\",\n",
      "                \"hp\",\n",
      "                \"macbook\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"macbook\": {\n",
      "        \"desc\": \"\",\n",
      "        \"id\": \"macbook\",\n",
      "        \"name\": \"\",\n",
      "        \"other\": {},\n",
      "        \"relations\": {\n",
      "            \"is_a\": [\n",
      "                \"laptop\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pronto\n",
    "ont = pronto.Ontology('/Users/kumarsanjeev/Desktop/Laptop Request.owl')\n",
    "#print(ont.obo)\n",
    "ontJSON = ont.json.lower()\n",
    "print(ontJSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connversion of .OWL into .JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "d = json.loads(ontJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'laptop'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RequestOffering=list1[-1].replace(' ' , ':').lower()\n",
    "RequestOffering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion of ontology term\n",
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dell', 'hp', 'macbook']\n",
      "I want to have new macbook .\n",
      "I want to have new hp .\n",
      "I need new macbook .\n",
      "I want new macbook .\n",
      "I need new macbook .\n",
      "I want to have new dell .\n",
      "I would like to have new hp .\n",
      "I want to have new hp .\n",
      "I would like to have new hp .\n",
      "I would like to have new dell .\n",
      "I want to have new hp .\n",
      "I want new hp .\n",
      "I would like to have new hp .\n",
      "I need new dell .\n",
      "I would like to have new dell .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "try:\n",
    "    b = d[RequestOffering][\"relations\"][\"can_be\"]\n",
    "    print(b)\n",
    "except KeyError:\n",
    "    print(\"Not Found\")\n",
    "    b=[]\n",
    "for _ in range(15):\n",
    "    if len(b) == 0:\n",
    "        print(\"Ontology Not Found then take user input\")\n",
    "        obj = [input().split(',')][0]\n",
    "        for _ in range(15):\n",
    "            subj = [\"I\"]\n",
    "            verb = [\"want\",\"need\",\"want to have\",\"would like to have\"]# Pattern words: User will have to provide keywords for the verbs to dentify the patterns. \n",
    "            adj = [\"new\"]\n",
    "            l = [subj,verb,adj,obj,\".\"] \n",
    "            m = ' '.join([random.choice(i) for i in l])\n",
    "            print(m)\n",
    "        break\n",
    "    else:\n",
    "        subj = [\"I\"]\n",
    "        verb = [\"want\",\"need\",\"want to have\",\"would like to have\"]# Pattern words: User will have to provide keywords for the verbs to dentify the patterns. \n",
    "        adj = [\"new\"]\n",
    "        obj = []\n",
    "        for i in b:\n",
    "            obj.append(i)\n",
    "\n",
    "        l=[subj,verb,adj,obj,\".\"] \n",
    "        m = ' '.join([random.choice(i) for i in l])\n",
    "        print (m.replace(\":\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, Laptop, 'want']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc1 = nlp(r\"I want Laptop\")\n",
    "\n",
    "list1 = []      \n",
    "for chunk in doc1.noun_chunks:\n",
    "    list1.append(chunk)\n",
    "list1.append(chunk.root.head.text)\n",
    "print (list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'want'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.root.head.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The boys, a new laptop]\n"
     ]
    }
   ],
   "source": [
    "# # Phrase Extraction \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc1 = nlp(u\"The boys need a new laptop\")\n",
    "doc = nlp(u\"I need to reset domain password\")\n",
    "#print (list(doc.noun_chunks))\n",
    "final_chunks = []\n",
    "for chunk in doc1.noun_chunks:\n",
    "    final_chunks.append(chunk)\n",
    "print (final_chunks)    \n",
    "    #print(\"Chunks are:\",final_chunks.append(chunk))\n",
    "#     print(chunk.root.dep_ ,chunk.root.text)\n",
    "#     if 'dobj'  in chunk.root.dep_:\n",
    "#         print(chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
