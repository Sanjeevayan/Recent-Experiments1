{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Request Offering: \"Request for an Equipment i.e. laptop\"\n",
    "\n",
    "# Using Gensim library & word2vec model(one of the most state of the art algo to find semantic similarity of terms)\n",
    "# Word2vec is one algorithm which uses shallow neural network for learning a word embedding from a text corpus.\n",
    "# This word2vec model has been trained on Google news data\n",
    "# It includes word vectors for a vocabulary of 3 million words and phrases \n",
    "# they were trained on roughly 100 billion words from a Google News dataset. The vector length is 300.\n",
    "\n",
    "import gensim\n",
    "\n",
    "#model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"/Users/kumarsanjeev/Downloads/GoogleNews-vectors-negative300.bin\",binary = True)\n",
    "#print(model.similar_by_word('laptop',topn=20))\n",
    "\n",
    "#print(model.similar_by_word('want' ,topn=20))\n",
    "# print(model.similar_by_word('projector' ,topn=20))\n",
    "#print(model.similar_by_word('laptop_computers' ,topn=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an entity:laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kumarsanjeev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('laptops', 0.805374026298523), ('laptop_computer', 0.7848465442657471), ('notebook', 0.67857825756073), ('netbook', 0.6707929372787476), ('computer', 0.6640493273735046), ('laptop_computers', 0.6633790731430054), ('notebook_PC', 0.6631842851638794), ('MacBook', 0.6598750352859497), ('PowerBook', 0.6520565152168274), ('Sony_Vaio_laptop', 0.6496157646179199), ('notebooks', 0.6491786241531372), ('Macbook', 0.6486797332763672), ('Laptop', 0.648013710975647), ('IBM_ThinkPad_T##', 0.6193387508392334), ('MacBook_Pro', 0.6187069416046143), ('Dell_Latitude_D###', 0.6164904832839966), ('Macbook_Pro', 0.6115914583206177), ('computers', 0.6102420091629028), ('Dell_Inspiron_laptop', 0.6090463399887085), ('notebook_computers', 0.607465386390686)]\n"
     ]
    }
   ],
   "source": [
    "# Automatic sentence generation using Entity & Taxonomy terms  through Word2vec model\n",
    "sent = \"I want a new laptop\".split()# summary/description provided by User\n",
    "entity = input (\"Enter an entity:\")\n",
    "# Entity & Taxonomy terms  Extraction through Word2vec model\n",
    "entity_list = model.wv.similar_by_word(entity ,topn=20)# word2vec Model extracts all the related terms (entity & taxonomy terms) in same semantic vector space.\n",
    "print (entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need new laptops .\n",
      "I want new computers .\n",
      "I need new notebook_computers .\n",
      "I would like to have new computer .\n",
      "I want to have new laptop_computers .\n",
      "I want to have new notebooks .\n",
      "I need new PowerBook .\n",
      "I would like to have new Sony_Vaio_laptop .\n",
      "I need new Macbook .\n",
      "I need new laptop_computer .\n",
      "I want to have new PowerBook .\n",
      "I want to have new notebook_PC .\n",
      "I want to have new laptop_computers .\n",
      "I want to have new MacBook .\n",
      "I need new notebooks .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for _ in range(15):\n",
    "    subj = [\"I\"]\n",
    "    verb = [\"want\",\"need\",\"want to have\",\"would like to have\"]# Pattern words: User will have to provide keywords for the verbs to dentify the patterns. \n",
    "    adj = [\"new\"]\n",
    "    obj = []#adds all the  entities and taxonomy terms extracted by word2vec Model\n",
    "    for i in entity_list:\n",
    "        obj.append(i[0])\n",
    "\n",
    "    l=[subj,verb,adj,obj,\".\"] \n",
    "    m = ' '.join([random.choice(i) for i in l])\n",
    "    print (m)\n",
    "\n",
    "    # Storing the output in a text file\n",
    "    s = s + m + ' \\n'\n",
    "    output_file = open(\"/Users/kumarsanjeev/Desktop/Test5.txt\")\n",
    "    path = \"/Users/kumarsanjeev/Desktop/Test5.txt\"\n",
    "    with open(path , 'w') as file:\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-420fc651dd31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Automatic sentence generation using Entity & Taxonomy terms  through Word2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#sent = \"I want a new laptop\".split()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter an entity:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Automatic sentence generation using Entity & Taxonomy terms  through Word2vec model\n",
    "#sent = \"I want a new laptop\".split()\n",
    "entity = input (\"Enter an entity:\")\n",
    "\n",
    "list1 = model.similar_by_word(entity ,topn=20)\n",
    "print (list1)\n",
    "# list2 = model.similar_by_word('want', topn=20)\n",
    "# for i, j in zip(list1,list2):\n",
    "#     print(\"i {} a {}\".format(j[0] ,i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity & Taxonomy terms  Extraction through Word2vec model\n",
    "import random\n",
    "#sent = \"I want a laptop\".split()\n",
    "list1 = model.similar_by_word('laptop' ,topn=20)\n",
    "#list2 = model.similar_by_word('want', topn=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to have IBM_ThinkPad_T## .\n"
     ]
    }
   ],
   "source": [
    "# Sentence Generation for the  pattern : Subject + Verb + Object\n",
    "subj = [\"I\"]\n",
    "verb = [\"want\",\"need\",\"want to have\",\"would like to have\"]# User will have tp provide keywords for the verbs to dentify the patterns. \n",
    "obj = []\n",
    "for i in list1:\n",
    "    obj.append(i[0])\n",
    "\n",
    "l=[subj,verb,obj,\".\"] \n",
    "m = ' '.join([random.choice(i) for i in l])\n",
    "print (m)\n",
    "\n",
    "# Storing the output in a text file\n",
    "s = s + m + ' \\n'\n",
    "output_file = open(\"/Users/kumarsanjeev/Desktop/Test5.txt\")\n",
    "path = \"/Users/kumarsanjeev/Desktop/Test5.txt\"\n",
    "with open(path , 'w') as file:\n",
    "    file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the output in a text file\n",
    "s = s + m + ' \\n'\n",
    "output_file = open(\"/Users/kumarsanjeev/Desktop/Test5.txt\")\n",
    "path = \"/Users/kumarsanjeev/Desktop/Test5.txt\"\n",
    "with open(path , 'w') as file:\n",
    "    file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a pronoun:I\n",
      "enter a pronoun:We\n",
      "enter a pronoun:They\n",
      "['I', 'We', 'They']\n"
     ]
    }
   ],
   "source": [
    "# User Input for subject\n",
    "subj = []\n",
    "for i in range(3):\n",
    "    subject = input(\"enter a pronoun:\")\n",
    "    subj.append(subject)\n",
    "\n",
    "    print (subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an entity:equipment\n",
      "[('equipments', 0.7184020280838013), ('Equipment', 0.6507868766784668), ('equiment', 0.64060378074646), ('equip_ment', 0.6304388046264648), ('equiptment', 0.6143404245376587), ('machinery', 0.59406578540802), ('equipement', 0.5912227630615234), ('eqipment', 0.5761674642562866), ('cranes_forklifts', 0.5646820664405823), ('upgrade_Fortkamp', 0.5597618222236633), ('wooden_sarcophagus_containing', 0.5492346286773682), ('gear', 0.5489619374275208), ('equpment', 0.546511173248291), ('manager_Rob_Cucuzza', 0.533706784248352), ('professionals_ELFA_SmartBrief', 0.518406867980957), ('manager_Dwayne_Mandrusiak', 0.5170284509658813), ('hoses_nozzles', 0.5133376121520996), ('digger_derricks', 0.5107513666152954), ('compressors_generators', 0.5076912641525269), ('Dana_Heinze', 0.5019118785858154)]\n"
     ]
    }
   ],
   "source": [
    "# User Input for entity\n",
    "sent = \"I want a new laptop\".split()\n",
    "entity = input (\"Enter an entity:\")\n",
    "\n",
    "\n",
    "list1 = model.similar_by_word(entity ,topn=20)\n",
    "print (list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next week DATE\n",
      "Madrid GPE\n",
      "SAP ORG\n"
     ]
    }
   ],
   "source": [
    "#Spacy NER\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"\"\"\"Next week I'll be in Madrid.I want a new computer.I want to loan a projector.I need to reset my domain password.I need to restore my desktop.I need to have my account unlocked.I want to take backup of files.I need an email alias.I would like to have an email account.I want to have account for SAP.\"\"\")\n",
    "# for line in doc:\n",
    "#     print (line)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : Mark, , Entity : PERSON\n",
      "Word : Elliot, , Entity : PERSON\n",
      "Word : Zuckerberg, , Entity : PERSON\n",
      "Word : (, , Entity : \n",
      "Word : born, , Entity : \n",
      "Word : May, , Entity : DATE\n",
      "Word : 14, , Entity : DATE\n",
      "Word : ,, , Entity : DATE\n",
      "Word : 1984, , Entity : DATE\n",
      "Word : ), , Entity : \n",
      "Word : is, , Entity : \n",
      "Word : a, , Entity : \n",
      "Word : co, , Entity : \n",
      "Word : -, , Entity : \n",
      "Word : founder, , Entity : \n",
      "Word : of, , Entity : \n",
      "Word : Facebook, , Entity : ORG\n",
      "Word : ., , Entity : \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_fr = spacy.load(\"en\")\n",
    "\n",
    "# We create a sentence\n",
    "text_en = \"Mark Elliot Zuckerberg (born May 14, 1984) is a co-founder of Facebook.\"\n",
    "\n",
    "# We process the sentence through the pipeline\n",
    "doc_en = nlp_fr(text_en)\n",
    "\n",
    "# We print each token and if an entity is recognized, we print the entity type\n",
    "for token in doc_en:\n",
    "#     print (token.text,token.ent_type_)\n",
    "    print('Word : {0}, , Entity : {1}' .format(token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
