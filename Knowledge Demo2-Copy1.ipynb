{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how do  i  delete  internet cookies?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing  characters\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "\n",
    "    # remove HTML tags\n",
    "    #text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "\n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "print(clean_text(\"How do \\ I '' delete \"\" internet cookies?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens are [('Am', 'NNP'), ('I', 'PRP'), ('entitled', 'VBD'), ('for', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('MacBook', 'NNP'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize,pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "q_words = [\"who\", \"what\", \"where\", \"how\",\"are\",\"am\",\"is\",\"can\"]\n",
    "q_symbols = [\"?\"]\n",
    "wh_list = [\"WP\", \"WRB\",\"VBP\",\"VBZ\"]\n",
    "stop_words = [\"a\",\"the\"]\n",
    "#sample = \"<h1>Am I [###]  service requests for Macbook</h1>\"\n",
    "sample1 = \"Am I entitled for a new MacBook? \"\n",
    "sample2 = \"I like my new laptop.\"\n",
    "sample3 = \"what is a domain account for MacBook?\"\n",
    "\n",
    "# NLP Based Text processing1:Tokenization , Parts of speech Tagger,Phrase Chunking,NER\n",
    "\n",
    "#1.NLP Function to  tokenize the string\n",
    "\n",
    "def tagText(text):\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    taggedtokens = pos_tag(tokens)\n",
    "\n",
    "    return taggedtokens\n",
    "\n",
    "print (\"Tokens are\",tagText(sample2))\n",
    "\n",
    "#2.NLP Function to lemmatize the tokens present in the string\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas are: ['Am', 'I', 'entitle', 'for', 'a', 'new', 'MacBook', '?']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_verbs(text):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens1= word_tokenize(text)\n",
    "    lemmas = []\n",
    "\n",
    "    for word in tokens1:\n",
    "        lemma = lemmatizer.lemmatize(word,pos=\"v\")\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "print (\"Lemmas are:\",lemmatize_verbs(sample1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean text: ['Am', 'I', 'entitled', 'for', 'new', 'MacBook', '?']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    tokens1 = word_tokenize(text)\n",
    "    new_words = []\n",
    "    for word in tokens1:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "print (\"Clean text:\",remove_stopwords(sample1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#4. NLP Function to identify pharses present in the string\n",
    "\n",
    "# def phrase_identify(text):\n",
    "#     extractor = ConllExtractor()\n",
    "#     blob = TextBlob(text,np_extractor=extractor)\n",
    "\n",
    "#     return (blob.noun_phrases)\n",
    "\n",
    "\n",
    "# print (\"Phrases are:\",phrase_identify(sample1))\n",
    "\n",
    "#5.  NLP Function to identify entities present in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Names are: []\n"
     ]
    }
   ],
   "source": [
    "#5.  NLP Function to identify entities present in the string\n",
    "def name_entity(text):\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    tagged_sentences = nltk.pos_tag(tokenized_text)\n",
    "    ne_chunked_sents = nltk.ne_chunk(tagged_sentences)\n",
    "    named_entities = []\n",
    "    for tagged_tree in ne_chunked_sents:\n",
    "        if hasattr(tagged_tree, 'label'):\n",
    "            entity_name = ' '.join(c[0] for c in tagged_tree.leaves())  #\n",
    "            entity_type = tagged_tree.label()  # get NE category\n",
    "            named_entities.append((entity_name, entity_type))\n",
    "\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "\n",
    "print (\"Entity Names are:\",name_entity(sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like my new laptop.: It is a (Knowledge) Question.\n"
     ]
    }
   ],
   "source": [
    "#6. Function to identify whether input string is question or not\n",
    "def identifyKnowledgeQuestion(taggedTokens):\n",
    "    flag = False\n",
    "\n",
    "    if (taggedTokens[0][1] in wh_list) or (taggedTokens[-1][0] in q_symbols):\n",
    "        flag = True\n",
    "        \n",
    "\n",
    "    return flag\n",
    "\n",
    "\n",
    "tagged_tokens = tagText(sample3)\n",
    "\n",
    "\n",
    "result = identifyKnowledgeQuestion(tagged_tokens)\n",
    "\n",
    "\n",
    "if (result == True):\n",
    "    print(str(sample2) + \": It is a (Knowledge) Question.\")\n",
    "else:\n",
    "    print(str(sample2) + \": It is not  a (Knowledge) Question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
